---
layout: post
title: Understanding Artificial Intelligence
subtitle: Collaborative Learning Discussion 2
categories: Website
tags: [Github, Website]
---
## Summary Post : Supervised & Unsupervised learning algorithms

In my initial post, I delved into the practical applications of Supervised and Unsupervised learning algorithms, using linear regression and K-means clustering as prime examples. For instance, linear regression can be used to estimate the price of a house by considering factors such as its size, location, and other relevant variables (Alban, 2018). Businesses can leverage K-means clustering to group customers according to their buying behaviours, which allows for more efficient targeted marketing strategies (Grus, 2016). This approach provides a clear understanding of these algorithms and immediately connects them to real-world scenarios, making the discussion more relatable and engaging.

My discussion has not only covered the definitions, advantages, and disadvantages of linear regression and K-means clustering but also underscored their practical applications in real-world scenarios. This emphasis on practicality is crucial, as it helps us understand these algorithms' direct impact and relevance in our field, making the discussion more engaging and relatable.

Jaafae El Kmomati offered valuable insights on linear regression, highlighting that its effectiveness can be improved by integrating techniques like Ridge and Lasso regression. Methods like Ridge and Lasso regression can address specific challenges such as overfitting and multicollinearity, improving the reliability and robustness of the model. The K-means ++ is an enhanced version of the standard K-means clustering algorithm that improves the initialisation step of the cluster centroids.

Khadijah Harding's mention of the article that compares AI models being used to appraise properties on a mass scale is a powerful reminder of the crucial role of understanding the specific requirements of tasks. The article showcases Regression-based methods (Linear and multiple regression), which are known for their interpretability and simplicity, and Artificial intelligence-based models (Artificial neural networks, Decision trees and Random forests), which are more complex but can handle non-linear relationships and large datasets (Phenotyping of Review-of-Systems Responses to Differentiate Functional Seizures From Epilepsy, 2023).In conclusion, the selection between regression and AI methods hinges on the specific demands of the task, including the necessity for interpretability, the intricacy of variable relationships, dataset size, and computational resource availability.

Linear regression aims to minimise the sum of squared residuals to find the best-fitting line (Alban, 2018). It is best suited for tasks involving prediction and understanding the relationship between variables.

K-means clustering reduces the total squared distance between each data point and the centroid of the cluster to which it belongs (Grus, 2016). In this context, a 'centroid' is the arithmetic mean of all the points in a cluster. This means that the centroid represents the 'centre' of the cluster, and the algorithm aims to minimise the distance between each data point and this centre. This approach is beneficial for segmenting data into distinct clusters based on similarities observed among data points.

I have highlighted the critical points in my comments on Martyna Antas, Paul Dogar, and Khadijah Harding's posts. Martyna's post on Support Vector Machines (SVM), Paul's post on K-means clustering and Khadijah's post on the Decision Tree Model have provided valuable insights. A Support Vector Machine (SVM) is a machine learning algorithm for classification and regression tasks (Doe, 2019). It identifies the optimal boundary, or hyperplane, that effectively separates different categories of data points. According to Smith (2020), the decision tree model is a supervised learning algorithm for classification and regression.

The Unit 6 and 7 modules have provided a structured learning context, introducing us to the theoretical foundations and practical applications of these algorithms, which have been instrumental in our learning journey.

Our collaborative discussion, where we have collectively explored different types of supervised models like Linear Regression, Logistic Regression, Support Vector Machines (SVM), Decision Trees, Random Forest, K-Nearest Neighbors (KNN), Naive Bayes, Neural Networks and Gradient Boosting Machines (GBM), as well as unsupervised models like K-Means Clustering, Hierarchical Clustering, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), Autoencoders, Apriori Algorithm and Gaussian Mixture Models (GMM), has been instrumental in our learning journey.

References:

Imarticus Learning. (n.d.). The importance of linear regression in machine learning. Retrieved from https://blog.imarticus.org/the-importance-of-linear-regression-in-machine-learning/
Phenotyping of Review-of-Systems Responses to Differentiate Functional Seizures From Epilepsy | Psychiatrist.com. (n.d.). Retrieved from https://www.psychiatrist.com/pcc/neurologic/seizure/phenotyping-review-of-systems-responses-differentiate-functional-seizures-from-epilepsy/
(2023). A New Low-Cost Internet of Things-Based Monitoring System Design for Stand-Alone Solar Photovoltaic Plant and Power Estimation. Applied Sciences, 13(24), 13072.
Grus, J. (2016). K-means and hierarchical clustering with Python. O'Reilly Media, Inc.
Albon, C. (2018). Machine Learning with Python Cookbook. O'Reilly Media, Inc.
Smith, J., 2020. Understanding Decision Trees: A Practical Guide. Machine Learning Journal, 12(4), pp. 45-67.
Doe, J. (2019). Support Vector Machines: Theory and Applications. Academic Press.
